# Machine Learning Model

A robust machine learning model with comprehensive cross-validation evaluation and consistent performance across multiple folds.

## ðŸŒŸ Competition Overview

This model was developed for a Kaggle competition hosted by the **Child Mind Institute**, held between **May 30 to August 27** (3 months). The competition aimed at advancing gesture recognition through wearable sensor data.

**Prizes & Awards:**

* Total prize pool: **\$50,000**
* Awards: **Points & Medals** for top-performing teams

## ðŸŽ¯ Performance Overview

| Metric                  | Score      | Standard Deviation |
| ----------------------- | ---------- | ------------------ |
| **Test Score**          | **74.01%** | Â±1.53%             |
| **Test Accuracy**       | **59.72%** | Â±1.97%             |
| **Validation Score**    | **73.45%** | Â±1.45%             |
| **Validation Accuracy** | **58.95%** | Â±2.11%             |

## ðŸ“Š Cross-Validation Results

The model was evaluated using **5-fold cross-validation** to ensure robust performance assessment:

### Fold-by-Fold Performance

| Fold | Val Accuracy | Val Score | Test Accuracy | Test Score |
| ---- | ------------ | --------- | ------------- | ---------- |
| 1    | 63.09%       | 76.19%    | 63.55%        | **76.93%** |
| 2    | 58.52%       | 73.32%    | 59.31%        | 73.35%     |
| 3    | 57.48%       | 72.94%    | 58.34%        | 73.81%     |
| 4    | 57.48%       | 71.87%    | 58.15%        | 72.42%     |
| 5    | 58.19%       | 72.95%    | 59.23%        | 73.55%     |

## âœ… Key Findings

* **ðŸŽ¯ Consistent Performance**: Low standard deviation (< 2%) across all folds indicates model stability
* **ðŸš« No Overfitting**: Test performance matches validation performance, showing good generalization
* **ðŸ“ˆ Reliable Results**: Small variance across folds demonstrates reproducible performance
* **ðŸ† Best Performance**: Fold 1 achieved the highest test score of 76.93%

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py          # Model architecture
â”‚   â”œâ”€â”€ train.py          # Training script
â”‚   â””â”€â”€ evaluate.py       # Evaluation utilities
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/            # Training data
â”‚   â””â”€â”€ test/             # Test data
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ model_checkpoint.pth
â”œâ”€â”€ results/
â”‚   â””â”€â”€ kfold_results.json
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ðŸ”§ Model Details

* **Architecture**: \[Add your model architecture details]
* **Training Strategy**: 5-fold Cross-Validation
* **Evaluation Metrics**: Custom score metric and accuracy
* **Framework**: PyTorch
* **Device Support**: CUDA/CPU compatible

## ðŸ› ï¸ Technical Notes

### Loading Checkpoints

Due to PyTorch 2.6+ security changes, use:

```python
checkpoint = torch.load(model_path, map_location=device, weights_only=False)
```

Or for safer loading:

```python
import torch.serialization
with torch.serialization.safe_globals([numpy.core.multiarray.scalar]):
    checkpoint = torch.load(model_path, map_location=device)
```



## ðŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


## ðŸ™ Acknowledgments

* \[Contest Link](https://www.kaggle.com/competitions/cmi-detect-behavior-with-sensor-data)

---

**Note**: Replace placeholder values like `YourModelClass`, `yourusername`, and file paths with your actual project details.
